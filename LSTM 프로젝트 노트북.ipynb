{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM을 활용한 자연어 처리"
      ],
      "metadata": {
        "id": "8IpwHkC-0gKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리"
      ],
      "metadata": {
        "id": "jytSK_pSwsU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋: IMDB 영화 리뷰 데이터\n",
        "\n",
        "목표: 영화 리뷰가 긍정적인지 부정적인지 분류하는 신경망 개발\n",
        "\n",
        "접근 방식: 자연어 처리에 적합한 RNN (순환 신경망) 사용. RNN 기법 중 LSTM 기법 활용."
      ],
      "metadata": {
        "id": "5fX95N0QwwL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**주의**: 해당 코드는 파이토치 1.11.0 버전, torchtext 0.12.0 버전, torchdata 0.3.0 버전을 사용. 아래의 코드를 사용하여 해당 버전들을 사용할수 있음. (주석 처리 되어있다면 해제하고 사용)"
      ],
      "metadata": {
        "id": "KeBUUnD-y7zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0 torchtext==0.12.0 torchdata==0.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI396eSpy5aN",
        "outputId": "d95d4898-444a-4b86-c034-1cb64add4084"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.12.0\n",
            "  Downloading torchtext-0.12.0-cp310-cp310-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.3.0\n",
            "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.25.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.3.0) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2024.6.2)\n",
            "Installing collected packages: torch, torchtext, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0 torchdata-0.3.0 torchtext-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t1CrGjDFtl10"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import string\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchdata\n",
        "import torchtext\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 설정\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# 어휘 생성 함수\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# IMDB 데이터셋 로드\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "\n",
        "# 어휘 생성\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), max_tokens=10000, specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "YBna6BJ7yvi1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어 제거를 위해 nltk를 사용합니다."
      ],
      "metadata": {
        "id": "7p-nNRy_7Qm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n17DDoVh7UUU",
        "outputId": "bc8721d2-50fa-47b4-a6de-f9ed34ec06c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Sn005TgPPUv5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리 및 레이블 변환\n",
        "def text_pipeline(x):\n",
        "    text = [token.lower() for token in tokenizer(x)]\n",
        "    text = [token.replace(\"<br\", \"\") for token in text]\n",
        "    text = [''.join(c for c in token if c not in string.punctuation) for token in text]\n",
        "    text = [token for token in text if token]\n",
        "    text = [token for token in text if not token in stopwords]\n",
        "    return text[:100] + ['<pad>'] * (100 - len(text))\n",
        "\n",
        "def label_pipeline(x):\n",
        "    return 1 if x == 'pos' else 0"
      ],
      "metadata": {
        "id": "SMq91oYb0Vo1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB 데이터셋을 커스텀 Dataset으로 변환\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_iter, text_transform, label_transform):\n",
        "        self.data = list(data_iter)\n",
        "        self.text_transform = text_transform\n",
        "        self.label_transform = label_transform\n",
        "        self.preprocessed_data = [\n",
        "            {'text': self.text_transform(text), 'label': self.label_transform(label)}\n",
        "            for label, text in self.data\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.preprocessed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.preprocessed_data[idx]\n",
        "\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "train_dataset = IMDBDataset(train_iter, text_pipeline, label_pipeline)\n",
        "test_dataset = IMDBDataset(test_iter, text_pipeline, label_pipeline)"
      ],
      "metadata": {
        "id": "xbsahdZ00Wdc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 분할\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_data, valid_data = random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "# 데이터 로더를 위한 collate 함수 설정\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for example in batch:\n",
        "        label_list.append(example['label'])\n",
        "        text_list.append(torch.tensor([vocab[token] for token in example['text']], dtype=torch.long))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
        "    text_list = pad_sequence(text_list, batch_first=False, padding_value=vocab[\"<pad>\"])\n",
        "    return text_list, label_list"
      ],
      "metadata": {
        "id": "jxDA0ju81DOg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로더 생성\n",
        "BATCH_SIZE = 50\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# 데이터 로더 사용 예시\n",
        "for batch in train_dataloader:\n",
        "    texts, labels = batch\n",
        "    print(f\"Labels: {labels}\")\n",
        "    print(f\"Texts: {texts}\")\n",
        "    break\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_dataset)}')\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDVkXZtR1Ros",
        "outputId": "eae84c42-58b9-4ae9-cf48-9efb0975ddba"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0])\n",
            "Texts: tensor([[  81,   23,  472,  ..., 1213,  475, 3877],\n",
            "        [  45,  589,  113,  ...,  332, 3214, 7343],\n",
            "        [4392,  715,   21,  ...,  587,  711,  420],\n",
            "        ...,\n",
            "        [   0,  354,  487,  ...,    1,  230,    1],\n",
            "        [4030,  540,   89,  ...,    1, 7838,    1],\n",
            "        [1656,  116,  487,  ...,    1,  194,    1]])\n",
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n",
            "Unique tokens in TEXT vocabulary: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM 사용"
      ],
      "metadata": {
        "id": "uvJ1tTcQ1Wcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LABEL vocabulary는 이미 0과 1로 구성된 숫자로 처리됩니다.\n",
        "label_vocab = {0: 'neg', 1: 'pos'}\n",
        "print(label_vocab)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = self.embedding(x)\n",
        "        x, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "        x = self.dropout(x[:, -1, :])  # We only take the output of the last LSTM cell\n",
        "        x = self.fc(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x, (hidden, cell)\n",
        "\n",
        "    def init_hidden_and_cell(self, batch_size):\n",
        "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
        "        cell = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(device)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "-NmdT1PE1V9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce3d345-e9a5-4f31-cb46-db7132b605cb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'neg', 1: 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emRvovqfH_6Y",
        "outputId": "b0d0cf2e-a8eb-4e74-9f10-cd9c4847ee3c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_size = 100\n",
        "hidden_size = 300\n",
        "num_layers = 1\n",
        "output_size = 2\n",
        "dropout = 0.3\n",
        "\n",
        "model = LSTMModel(vocab_size, embed_size, hidden_size, output_size, num_layers, dropout)\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def training(epoch, model, trainloader, validloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    hidden, cell = model.init_hidden_and_cell(BATCH_SIZE)\n",
        "\n",
        "    for texts, labels in trainloader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        texts = torch.transpose(texts, 0, 1).contiguous()\n",
        "\n",
        "        hidden, cell = hidden.detach(), cell.detach()\n",
        "\n",
        "        y_pred, (hidden, cell) = model(texts, hidden, cell)\n",
        "\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            correct += (y_pred.max(1)[1].view(labels.size()).data == labels.data).sum().item()\n",
        "            total += labels.size(0)\n",
        "            running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(trainloader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    valid_correct = 0\n",
        "    valid_total = 0\n",
        "    valid_running_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # hidden, cell = model.init_hidden_and_cell(BATCH_SIZE)\n",
        "\n",
        "        for texts, labels in validloader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            texts = torch.transpose(texts, 0, 1).contiguous()\n",
        "            y_pred, (hidden, cell) = model(texts, hidden, cell)\n",
        "            loss = loss_fn(y_pred, labels)\n",
        "            valid_correct += (y_pred.max(1)[1].view(labels.size()).data == labels.data).sum().item()\n",
        "            valid_total += labels.size(0)\n",
        "            valid_running_loss += loss.item()\n",
        "\n",
        "    epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
        "    epoch_valid_acc = valid_correct / valid_total\n",
        "\n",
        "    print('epoch: ', epoch,\n",
        "          'loss： ', round(epoch_loss, 3),\n",
        "          'accuracy:', round(epoch_acc, 3),\n",
        "          'valid_loss： ', round(epoch_valid_loss, 3),\n",
        "          'valid_accuracy:', round(epoch_valid_acc, 3)\n",
        "          )\n",
        "    return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc\n",
        "\n",
        "epochs = 10\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(epoch, model, train_dataloader, valid_dataloader)\n",
        "\n",
        "    if epoch_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = epoch_valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "\n",
        "    train_loss.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    valid_loss.append(epoch_valid_loss)\n",
        "    valid_acc.append(epoch_valid_acc)"
      ],
      "metadata": {
        "id": "ewUhYnse28K0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748af54b-756b-435b-9d1f-80845026b6b8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 loss：  0.014 accuracy: 0.504 valid_loss：  0.014 valid_accuracy: 0.509\n",
            "epoch:  1 loss：  0.014 accuracy: 0.536 valid_loss：  0.013 valid_accuracy: 0.62\n",
            "epoch:  2 loss：  0.012 accuracy: 0.677 valid_loss：  0.014 valid_accuracy: 0.552\n",
            "epoch:  3 loss：  0.011 accuracy: 0.776 valid_loss：  0.011 valid_accuracy: 0.773\n",
            "epoch:  4 loss：  0.01 accuracy: 0.822 valid_loss：  0.01 valid_accuracy: 0.789\n",
            "epoch:  5 loss：  0.009 accuracy: 0.853 valid_loss：  0.01 valid_accuracy: 0.795\n",
            "epoch:  6 loss：  0.009 accuracy: 0.873 valid_loss：  0.01 valid_accuracy: 0.806\n",
            "epoch:  7 loss：  0.009 accuracy: 0.882 valid_loss：  0.01 valid_accuracy: 0.805\n",
            "epoch:  8 loss：  0.008 accuracy: 0.895 valid_loss：  0.01 valid_accuracy: 0.806\n",
            "epoch:  9 loss：  0.008 accuracy: 0.903 valid_loss：  0.01 valid_accuracy: 0.812\n"
          ]
        }
      ]
    }
  ]
}